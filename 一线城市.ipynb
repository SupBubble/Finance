{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAL Model \n",
    "## 1. 天津"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://stj.speiyou.com/search/index/gtype:time/grade:-8/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_pages_year = []\n",
    "year_list = [-8,-7,-6,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "\n",
    "for j in year_list:\n",
    "    quote_page = 'http://stj.speiyou.com/search/index/gtype:time/grade:'+str(j)+'/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage'\n",
    "    quote_pages_year.append(quote_page)\n",
    "quote_pages_year[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_pages = []\n",
    "for i in range(1,2):\n",
    "    quote_page = quote_pages_year[0]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,5):\n",
    "    quote_page = quote_pages_year[1]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,16):\n",
    "    quote_page = quote_pages_year[2]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,42):\n",
    "    quote_page = quote_pages_year[3]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,51):\n",
    "    quote_page = quote_pages_year[4]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,56):\n",
    "    quote_page = quote_pages_year[5]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,53):\n",
    "    quote_page = quote_pages_year[6]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,42):\n",
    "    quote_page = quote_pages_year[7]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,33):\n",
    "    quote_page = quote_pages_year[8]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,42):\n",
    "    quote_page = quote_pages_year[9]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,42):\n",
    "    quote_page = quote_pages_year[10]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,35):\n",
    "    quote_page = quote_pages_year[11]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,22):\n",
    "    quote_page = quote_pages_year[12]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,13):\n",
    "    quote_page = quote_pages_year[13]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,2):\n",
    "    quote_page = quote_pages_year[14]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,6):\n",
    "    quote_page = quote_pages_year[15]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,4):\n",
    "    quote_page = quote_pages_year[16]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "print(len(quote_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Courses = []\n",
    "Date = []\n",
    "Time = []\n",
    "Price = []\n",
    "Subject = []\n",
    "Year = []\n",
    "for pg in quote_pages:\n",
    "    resp=urlopen(pg)\n",
    "    soup=BeautifulSoup(resp,'html.parser')\n",
    "    for row in soup.findAll('p'):\n",
    "    #print(row)\n",
    "        subject = re.findall('学科.{4}',str(row))\n",
    "        year = re.findall('<span>年级.{3,6}',str(row))\n",
    "        date = re.findall('开课日期.{22}',str(row))\n",
    "        time = re.findall('上课时间.{3,19}',str(row))\n",
    "    \n",
    "        subject = ''.join(subject)\n",
    "        year = ''.join(year)\n",
    "        date = ''.join(date)\n",
    "        time = ''.join(time)\n",
    "    \n",
    "        year = year.strip('<span></sp')\n",
    "    \n",
    "        Year.append(year)\n",
    "        Subject.append(subject)\n",
    "        Date.append(date)\n",
    "        Time.append(time)\n",
    "        \n",
    "    for row in soup.findAll('div',attrs={'class':'price'}):\n",
    "        price = re.findall('<span>.*</',str(row))\n",
    "        price = ''.join(price)\n",
    "        price = price.strip('<span></')\n",
    "        Price.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delet empty list from list\n",
    "Date = [x for x in Date if x]\n",
    "Time = [x for x in Time if x]\n",
    "Subject = [x for x in Subject if x]\n",
    "Year = [x for x in Year if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "XES = pd.DataFrame([Subject,Year,Date,Time,Price],index = ['Subject','Year','Date','Time','Price']).T\n",
    "writer = pd.ExcelWriter('学而思_天津数据.xlsx')\n",
    "XES.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 上海"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_pages = []\n",
    "for i in range(1, 932):\n",
    "    quote_page = 'http://ssh.speiyou.com/search/sk/?kw=&grade=&lesson=&level=&gtype=time&subject=&term=&service=&m=&d=&bg=n&nu=&curpage='+str(i)\n",
    "    quote_pages.append(quote_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Courses = []\n",
    "Date = []\n",
    "Time = []\n",
    "Price = []\n",
    "Subject = []\n",
    "Year = []\n",
    "for pg in quote_pages:\n",
    "    resp=urlopen(pg)\n",
    "    soup=BeautifulSoup(resp,'html.parser')\n",
    "    for row in soup.findAll('p'):\n",
    "    #print(row)\n",
    "        subject = re.findall('学科.{4}',str(row))\n",
    "        year = re.findall('<span>年级.{3,6}',str(row))\n",
    "        date = re.findall('开课日期.{22}',str(row))\n",
    "        time = re.findall('上课时间.{3,19}',str(row))\n",
    "    \n",
    "        subject = ''.join(subject)\n",
    "        year = ''.join(year)\n",
    "        date = ''.join(date)\n",
    "        time = ''.join(time)\n",
    "    \n",
    "        year = year.strip('<span></sp')\n",
    "    \n",
    "        Year.append(year)\n",
    "        Subject.append(subject)\n",
    "        Date.append(date)\n",
    "        Time.append(time)\n",
    "        \n",
    "    for row in soup.findAll('div',attrs={'class':'price'}):\n",
    "        price = re.findall('<span>.*</',str(row))\n",
    "        price = ''.join(price)\n",
    "        price = price.strip('<span></')\n",
    "        Price.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delet empty list from list\n",
    "Date = [x for x in Date if x]\n",
    "Time = [x for x in Time if x]\n",
    "Subject = [x for x in Subject if x]\n",
    "Year = [x for x in Year if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "XES = pd.DataFrame([Year,Subject,Date,Time,Price],index = ['Year','Subject','Date','Time','Price']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('学而思数据_上海.xlsx')\n",
    "XES.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 广州"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://sgz.speiyou.com/search/index/gtype:time/grade:-8/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage:'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_pages_year = []\n",
    "year_list = [-8,-7,-6,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n",
    "\n",
    "for j in year_list:\n",
    "    quote_page = 'http://sgz.speiyou.com/search/index/gtype:time/grade:'+str(j)+'/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage:'   \n",
    "    quote_pages_year.append(quote_page)\n",
    "quote_pages_year[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "quote_pages = []\n",
    "for i in range(1,4):\n",
    "    quote_page = quote_pages_year[0]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,4):\n",
    "    quote_page = quote_pages_year[1]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,15):\n",
    "    quote_page = quote_pages_year[2]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,40):\n",
    "    quote_page = quote_pages_year[3]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,61):\n",
    "    quote_page = quote_pages_year[4]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,73):\n",
    "    quote_page = quote_pages_year[5]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,90):\n",
    "    quote_page = quote_pages_year[6]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,102):\n",
    "    quote_page = quote_pages_year[7]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,80):\n",
    "    quote_page = quote_pages_year[8]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,63):\n",
    "    quote_page = quote_pages_year[9]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,86):\n",
    "    quote_page = quote_pages_year[10]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,85):\n",
    "    quote_page = quote_pages_year[11]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,44):\n",
    "    quote_page = quote_pages_year[12]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,32):\n",
    "    quote_page = quote_pages_year[13]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,12):\n",
    "    quote_page = quote_pages_year[14]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,7):\n",
    "    quote_page = quote_pages_year[15]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,3):\n",
    "    quote_page = quote_pages_year[16]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "print(len(quote_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Courses = []\n",
    "Date = []\n",
    "Time = []\n",
    "Price = []\n",
    "Subject = []\n",
    "Year = []\n",
    "for pg in quote_pages:\n",
    "    resp=urlopen(pg)\n",
    "    soup=BeautifulSoup(resp,'html.parser')\n",
    "    for row in soup.findAll('p'):\n",
    "    #print(row)\n",
    "        subject = re.findall('学科.{4}',str(row))\n",
    "        year = re.findall('<span>年级.{3,6}',str(row))\n",
    "        date = re.findall('开课日期.{22}',str(row))\n",
    "        time = re.findall('上课时间.{3,19}',str(row))\n",
    "    \n",
    "        subject = ''.join(subject)\n",
    "        year = ''.join(year)\n",
    "        date = ''.join(date)\n",
    "        time = ''.join(time)\n",
    "    \n",
    "        year = year.strip('<span></sp')\n",
    "    \n",
    "        Year.append(year)\n",
    "        Subject.append(subject)\n",
    "        Date.append(date)\n",
    "        Time.append(time)\n",
    "        \n",
    "    for row in soup.findAll('div',attrs={'class':'price'}):\n",
    "        price = re.findall('<span>.*</',str(row))\n",
    "        price = ''.join(price)\n",
    "        price = price.strip('<span></')\n",
    "        Price.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delet empty list from list\n",
    "Date = [x for x in Date if x]\n",
    "Time = [x for x in Time if x]\n",
    "Subject = [x for x in Subject if x]\n",
    "Year = [x for x in Year if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "XES = pd.DataFrame([Year,Subject,Date,Time,Price],index = ['Year','Subject','Date','Time','Price']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('学而思数据_广州.xlsx')\n",
    "XES.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 南京"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://snj.speiyou.com/search/index/gtype:time/grade:-9/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage:'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_pages_year = []\n",
    "year_list = [-9,-8,-7,-6,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "\n",
    "for j in year_list:\n",
    "    quote_page = 'http://snj.speiyou.com/search/index/gtype:time/grade:'+str(j)+'/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage:'   \n",
    "    quote_pages_year.append(quote_page)\n",
    "quote_pages_year[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728\n"
     ]
    }
   ],
   "source": [
    "quote_pages = []\n",
    "for i in range(1,2):\n",
    "    quote_page = quote_pages_year[0]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,2):\n",
    "    quote_page = quote_pages_year[1]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,7):\n",
    "    quote_page = quote_pages_year[2]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,33):\n",
    "    quote_page = quote_pages_year[3]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,61):\n",
    "    quote_page = quote_pages_year[4]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,85):\n",
    "    quote_page = quote_pages_year[5]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,85):\n",
    "    quote_page = quote_pages_year[6]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,76):\n",
    "    quote_page = quote_pages_year[7]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,71):\n",
    "    quote_page = quote_pages_year[8]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,56):\n",
    "    quote_page = quote_pages_year[9]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,118):\n",
    "    quote_page = quote_pages_year[10]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,44):\n",
    "    quote_page = quote_pages_year[11]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,51):\n",
    "    quote_page = quote_pages_year[12]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,30):\n",
    "    quote_page = quote_pages_year[13]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,13):\n",
    "    quote_page = quote_pages_year[14]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,5):\n",
    "    quote_page = quote_pages_year[15]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,4):\n",
    "    quote_page = quote_pages_year[16]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,2):\n",
    "    quote_page = quote_pages_year[17]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,2):\n",
    "    quote_page = quote_pages_year[18]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "print(len(quote_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Courses = []\n",
    "Date = []\n",
    "Time = []\n",
    "Price = []\n",
    "Subject = []\n",
    "Year = []\n",
    "for pg in quote_pages:\n",
    "    resp=urlopen(pg)\n",
    "    soup=BeautifulSoup(resp,'html.parser')\n",
    "    for row in soup.findAll('p'):\n",
    "    #print(row)\n",
    "        subject = re.findall('学科.{4}',str(row))\n",
    "        year = re.findall('<span>年级.{3,6}',str(row))\n",
    "        date = re.findall('开课日期.{22}',str(row))\n",
    "        time = re.findall('上课时间.{3,19}',str(row))\n",
    "    \n",
    "        subject = ''.join(subject)\n",
    "        year = ''.join(year)\n",
    "        date = ''.join(date)\n",
    "        time = ''.join(time)\n",
    "    \n",
    "        year = year.strip('<span></sp')\n",
    "    \n",
    "        Year.append(year)\n",
    "        Subject.append(subject)\n",
    "        Date.append(date)\n",
    "        Time.append(time)\n",
    "        \n",
    "    for row in soup.findAll('div',attrs={'class':'price'}):\n",
    "        price = re.findall('<span>.*</',str(row))\n",
    "        price = ''.join(price)\n",
    "        price = price.strip('<span></')\n",
    "        Price.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delet empty list from list\n",
    "Date = [x for x in Date if x]\n",
    "Time = [x for x in Time if x]\n",
    "Subject = [x for x in Subject if x]\n",
    "Year = [x for x in Year if x]\n",
    "XES = pd.DataFrame([Year,Subject,Date,Time,Price],index = ['Year','Subject','Date','Time','Price']).T\n",
    "writer = pd.ExcelWriter('学而思数据_南京.xlsx')\n",
    "XES.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 西安"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://sxa.speiyou.com/search/index/gtype:time/grade:-7/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage:'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_pages_year = []\n",
    "year_list = [-7,-6,1,2,3,4,5,6,7,8,9,10,11,12,15]\n",
    "\n",
    "for j in year_list:\n",
    "    quote_page = 'http://sxa.speiyou.com/search/index/gtype:time/grade:'+str(j)+'/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage:'   \n",
    "    quote_pages_year.append(quote_page)\n",
    "quote_pages_year[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n"
     ]
    }
   ],
   "source": [
    "quote_pages = []\n",
    "for i in range(1,4):\n",
    "    quote_page = quote_pages_year[0]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,14):\n",
    "    quote_page = quote_pages_year[1]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,20):\n",
    "    quote_page = quote_pages_year[2]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,21):\n",
    "    quote_page = quote_pages_year[3]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,26):\n",
    "    quote_page = quote_pages_year[4]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,39):\n",
    "    quote_page = quote_pages_year[5]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,50):\n",
    "    quote_page = quote_pages_year[6]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,64):\n",
    "    quote_page = quote_pages_year[7]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,38):\n",
    "    quote_page = quote_pages_year[8]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,50):\n",
    "    quote_page = quote_pages_year[9]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,65):\n",
    "    quote_page = quote_pages_year[10]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,29):\n",
    "    quote_page = quote_pages_year[11]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,20):\n",
    "    quote_page = quote_pages_year[12]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,14):\n",
    "    quote_page = quote_pages_year[13]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,3):\n",
    "    quote_page = quote_pages_year[14]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "print(len(quote_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Courses = []\n",
    "Date = []\n",
    "Time = []\n",
    "Price = []\n",
    "Subject = []\n",
    "Year = []\n",
    "for pg in quote_pages:\n",
    "    resp=urlopen(pg)\n",
    "    soup=BeautifulSoup(resp,'html.parser')\n",
    "    for row in soup.findAll('p'):\n",
    "    #print(row)\n",
    "        subject = re.findall('学科.{4}',str(row))\n",
    "        year = re.findall('<span>年级.{3,6}',str(row))\n",
    "        date = re.findall('开课日期.{22}',str(row))\n",
    "        time = re.findall('上课时间.{3,19}',str(row))\n",
    "    \n",
    "        subject = ''.join(subject)\n",
    "        year = ''.join(year)\n",
    "        date = ''.join(date)\n",
    "        time = ''.join(time)\n",
    "    \n",
    "        year = year.strip('<span></sp')\n",
    "    \n",
    "        Year.append(year)\n",
    "        Subject.append(subject)\n",
    "        Date.append(date)\n",
    "        Time.append(time)\n",
    "        \n",
    "    for row in soup.findAll('div',attrs={'class':'price'}):\n",
    "        price = re.findall('<span>.*</',str(row))\n",
    "        price = ''.join(price)\n",
    "        price = price.strip('<span></')\n",
    "        Price.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delet empty list from list\n",
    "Date = [x for x in Date if x]\n",
    "Time = [x for x in Time if x]\n",
    "Subject = [x for x in Subject if x]\n",
    "Year = [x for x in Year if x]\n",
    "XES = pd.DataFrame([Year,Subject,Date,Time,Price],index = ['Year','Subject','Date','Time','Price']).T\n",
    "writer = pd.ExcelWriter('学而思数据_西安.xlsx')\n",
    "XES.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 杭州"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://sxa.speiyou.com/search/index/gtype:time/grade:-8/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage:'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_pages_year = []\n",
    "year_list = [-8,-7,-6,1,2,3,4,5,6,7,8,9,10,11,13,14,15]\n",
    "\n",
    "for j in year_list:\n",
    "    quote_page = 'http://sxa.speiyou.com/search/index/gtype:time/grade:'+str(j)+'/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage:'   \n",
    "    quote_pages_year.append(quote_page)\n",
    "quote_pages_year[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n"
     ]
    }
   ],
   "source": [
    "quote_pages = []\n",
    "for i in range(1,3):\n",
    "    quote_page = quote_pages_year[0]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,6):\n",
    "    quote_page = quote_pages_year[1]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,25):\n",
    "    quote_page = quote_pages_year[2]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,47):\n",
    "    quote_page = quote_pages_year[3]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,54):\n",
    "    quote_page = quote_pages_year[4]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,58):\n",
    "    quote_page = quote_pages_year[5]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,65):\n",
    "    quote_page = quote_pages_year[6]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,65):\n",
    "    quote_page = quote_pages_year[7]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,46):\n",
    "    quote_page = quote_pages_year[8]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,42):\n",
    "    quote_page = quote_pages_year[9]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,39):\n",
    "    quote_page = quote_pages_year[10]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,33):\n",
    "    quote_page = quote_pages_year[11]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,18):\n",
    "    quote_page = quote_pages_year[12]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,8):\n",
    "    quote_page = quote_pages_year[13]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,2):\n",
    "    quote_page = quote_pages_year[14]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,3):\n",
    "    quote_page = quote_pages_year[15]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,2):\n",
    "    quote_page = quote_pages_year[16]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "print(len(quote_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Courses = []\n",
    "Date = []\n",
    "Time = []\n",
    "Price = []\n",
    "Subject = []\n",
    "Year = []\n",
    "for pg in quote_pages:\n",
    "    resp=urlopen(pg)\n",
    "    soup=BeautifulSoup(resp,'html.parser')\n",
    "    for row in soup.findAll('p'):\n",
    "    #print(row)\n",
    "        subject = re.findall('学科.{4}',str(row))\n",
    "        year = re.findall('<span>年级.{3,6}',str(row))\n",
    "        date = re.findall('开课日期.{22}',str(row))\n",
    "        time = re.findall('上课时间.{3,19}',str(row))\n",
    "    \n",
    "        subject = ''.join(subject)\n",
    "        year = ''.join(year)\n",
    "        date = ''.join(date)\n",
    "        time = ''.join(time)\n",
    "    \n",
    "        year = year.strip('<span></sp')\n",
    "    \n",
    "        Year.append(year)\n",
    "        Subject.append(subject)\n",
    "        Date.append(date)\n",
    "        Time.append(time)\n",
    "        \n",
    "    for row in soup.findAll('div',attrs={'class':'price'}):\n",
    "        price = re.findall('<span>.*</',str(row))\n",
    "        price = ''.join(price)\n",
    "        price = price.strip('<span></')\n",
    "        Price.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delet empty list from list\n",
    "Date = [x for x in Date if x]\n",
    "Time = [x for x in Time if x]\n",
    "Subject = [x for x in Subject if x]\n",
    "Year = [x for x in Year if x]\n",
    "XES = pd.DataFrame([Year,Subject,Date,Time,Price],index = ['Year','Subject','Date','Time','Price']).T\n",
    "writer = pd.ExcelWriter('学而思数据_杭州.xlsx')\n",
    "XES.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 武汉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://swh.speiyou.com/search/index/gtype:time/grade:-6/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage:'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quote_pages_year = []\n",
    "year_list = [-6,1,2,3,4,5,6,7,8,9,10,11,12,15]\n",
    "\n",
    "for j in year_list:\n",
    "    quote_page = 'http://swh.speiyou.com/search/index/gtype:time/grade:'+str(j)+'/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage:'   \n",
    "    quote_pages_year.append(quote_page)\n",
    "quote_pages_year[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389\n"
     ]
    }
   ],
   "source": [
    "quote_pages = []\n",
    "for i in range(1,6):\n",
    "    quote_page = quote_pages_year[0]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,16):\n",
    "    quote_page = quote_pages_year[1]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,30):\n",
    "    quote_page = quote_pages_year[2]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,46):\n",
    "    quote_page = quote_pages_year[3]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,43):\n",
    "    quote_page = quote_pages_year[4]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,43):\n",
    "    quote_page = quote_pages_year[5]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,52):\n",
    "    quote_page = quote_pages_year[6]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,36):\n",
    "    quote_page = quote_pages_year[7]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,39):\n",
    "    quote_page = quote_pages_year[8]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,44):\n",
    "    quote_page = quote_pages_year[9]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,28):\n",
    "    quote_page = quote_pages_year[10]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,11):\n",
    "    quote_page = quote_pages_year[11]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,7):\n",
    "    quote_page = quote_pages_year[12]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,2):\n",
    "    quote_page = quote_pages_year[13]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "print(len(quote_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Courses = []\n",
    "Date = []\n",
    "Time = []\n",
    "Price = []\n",
    "Subject = []\n",
    "Year = []\n",
    "for pg in quote_pages:\n",
    "    resp=urlopen(pg)\n",
    "    soup=BeautifulSoup(resp,'html.parser')\n",
    "    for row in soup.findAll('p'):\n",
    "    #print(row)\n",
    "        subject = re.findall('学科.{4}',str(row))\n",
    "        year = re.findall('<span>年级.{3,6}',str(row))\n",
    "        date = re.findall('开课日期.{22}',str(row))\n",
    "        time = re.findall('上课时间.{3,19}',str(row))\n",
    "    \n",
    "        subject = ''.join(subject)\n",
    "        year = ''.join(year)\n",
    "        date = ''.join(date)\n",
    "        time = ''.join(time)\n",
    "    \n",
    "        year = year.strip('<span></sp')\n",
    "    \n",
    "        Year.append(year)\n",
    "        Subject.append(subject)\n",
    "        Date.append(date)\n",
    "        Time.append(time)\n",
    "        \n",
    "    for row in soup.findAll('div',attrs={'class':'price'}):\n",
    "        price = re.findall('<span>.*</',str(row))\n",
    "        price = ''.join(price)\n",
    "        price = price.strip('<span></')\n",
    "        Price.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delet empty list from list\n",
    "Date = [x for x in Date if x]\n",
    "Time = [x for x in Time if x]\n",
    "Subject = [x for x in Subject if x]\n",
    "Year = [x for x in Year if x]\n",
    "XES = pd.DataFrame([Year,Subject,Date,Time,Price],index = ['Year','Subject','Date','Time','Price']).T\n",
    "writer = pd.ExcelWriter('学而思数据_武汉.xlsx')\n",
    "XES.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 重庆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_pages_year = []\n",
    "year_list = [-7,-6,1,2,3,4,5,6,7,8,9,10,11,12,14,15]\n",
    "\n",
    "for j in year_list:\n",
    "    quote_page = 'http://swh.speiyou.com/search/index/gtype:time/grade:'+str(j)+'/subject:/level:bx/lesson:/term:/period:/teaid:/m:/d:/time:/bg:n/nu:/service:/curpage:'   \n",
    "    quote_pages_year.append(quote_page)\n",
    "quote_pages_year[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_pages = []\n",
    "for i in range(1,3):\n",
    "    quote_page = quote_pages_year[0]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,8):\n",
    "    quote_page = quote_pages_year[1]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,16):\n",
    "    quote_page = quote_pages_year[2]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,25):\n",
    "    quote_page = quote_pages_year[3]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,40):\n",
    "    quote_page = quote_pages_year[4]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,40):\n",
    "    quote_page = quote_pages_year[5]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,31):\n",
    "    quote_page = quote_pages_year[6]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,28):\n",
    "    quote_page = quote_pages_year[7]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,20):\n",
    "    quote_page = quote_pages_year[8]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,26):\n",
    "    quote_page = quote_pages_year[9]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,25):\n",
    "    quote_page = quote_pages_year[10]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,16):\n",
    "    quote_page = quote_pages_year[11]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,9):\n",
    "    quote_page = quote_pages_year[12]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,3):\n",
    "    quote_page = quote_pages_year[13]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,2):\n",
    "    quote_page = quote_pages_year[14]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "for i in range(1,2):\n",
    "    quote_page = quote_pages_year[15]+str(i)\n",
    "    quote_pages.append(quote_page)\n",
    "print(len(quote_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Courses = []\n",
    "Date = []\n",
    "Time = []\n",
    "Price = []\n",
    "Subject = []\n",
    "Year = []\n",
    "for pg in quote_pages:\n",
    "    resp=urlopen(pg)\n",
    "    soup=BeautifulSoup(resp,'html.parser')\n",
    "    for row in soup.findAll('p'):\n",
    "    #print(row)\n",
    "        subject = re.findall('学科.{4}',str(row))\n",
    "        year = re.findall('<span>年级.{3,6}',str(row))\n",
    "        date = re.findall('开课日期.{22}',str(row))\n",
    "        time = re.findall('上课时间.{3,19}',str(row))\n",
    "    \n",
    "        subject = ''.join(subject)\n",
    "        year = ''.join(year)\n",
    "        date = ''.join(date)\n",
    "        time = ''.join(time)\n",
    "    \n",
    "        year = year.strip('<span></sp')\n",
    "    \n",
    "        Year.append(year)\n",
    "        Subject.append(subject)\n",
    "        Date.append(date)\n",
    "        Time.append(time)\n",
    "        \n",
    "    for row in soup.findAll('div',attrs={'class':'price'}):\n",
    "        price = re.findall('<span>.*</',str(row))\n",
    "        price = ''.join(price)\n",
    "        price = price.strip('<span></')\n",
    "        Price.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delet empty list from list\n",
    "Date = [x for x in Date if x]\n",
    "Time = [x for x in Time if x]\n",
    "Subject = [x for x in Subject if x]\n",
    "Year = [x for x in Year if x]\n",
    "XES = pd.DataFrame([Year,Subject,Date,Time,Price],index = ['Year','Subject','Date','Time','Price']).T\n",
    "writer = pd.ExcelWriter('学而思数据_重庆.xlsx')\n",
    "XES.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. 深圳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_pages = []\n",
    "for i in range(1, 2060):\n",
    "    quote_page = 'http://ssz.speiyou.com/search/sk/?kw=&grade=&lesson=&level=&gtype=time&subject=&term=&service=&m=&d=&bg=n&nu=&curpage='+str(i)\n",
    "    quote_pages.append(quote_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionResetError",
     "evalue": "[Errno 54] Connection reset by peer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-328438ddd207>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquote_pages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mresp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msoup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#print(row)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/bs4/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'read'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m        \u001b[0;31m# It's a file-type object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mmarkup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         elif len(markup) <= 256 and (\n\u001b[1;32m    193\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34mb'<'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readall_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_readall_chunked\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m                 \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 54] Connection reset by peer"
     ]
    }
   ],
   "source": [
    "Courses = []\n",
    "Date = []\n",
    "Time = []\n",
    "Price = []\n",
    "Subject = []\n",
    "Year = []\n",
    "for pg in quote_pages:\n",
    "    resp=urlopen(pg)\n",
    "    soup=BeautifulSoup(resp,'html.parser')\n",
    "    for row in soup.findAll('p'):\n",
    "    #print(row)\n",
    "        subject = re.findall('学科.{4}',str(row))\n",
    "        year = re.findall('<span>年级.{3,6}',str(row))\n",
    "        date = re.findall('开课日期.{22}',str(row))\n",
    "        time = re.findall('上课时间.{3,19}',str(row))\n",
    "    \n",
    "        subject = ''.join(subject)\n",
    "        year = ''.join(year)\n",
    "        date = ''.join(date)\n",
    "        time = ''.join(time)\n",
    "    \n",
    "        year = year.strip('<span></sp')\n",
    "    \n",
    "        Year.append(year)\n",
    "        Subject.append(subject)\n",
    "        Date.append(date)\n",
    "        Time.append(time)\n",
    "        \n",
    "    for row in soup.findAll('div',attrs={'class':'price'}):\n",
    "        price = re.findall('<span>.*</',str(row))\n",
    "        price = ''.join(price)\n",
    "        price = price.strip('<span></')\n",
    "        Price.append(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delet empty list from list\n",
    "Date = [x for x in Date if x]\n",
    "Time = [x for x in Time if x]\n",
    "Subject = [x for x in Subject if x]\n",
    "Year = [x for x in Year if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XES = pd.DataFrame([Year,Subject,Date,Time,Price],index = ['Year','Subject','Date','Time','Price']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('学而思数据_深圳.xlsx')\n",
    "XES.to_excel(writer)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
